{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MyNLPToolBox import FilePickling as FP\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in this experiment we're gonna fine-tune the best models we got from the experiment 3, which Logistic Regression with Boolean TFIDF Embedder, without removing special characters & stopwords in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the pre-embedded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded obj/x_train_boolean.pkl\n",
      "loaded obj/x_test_boolean.pkl\n",
      "loaded obj/y_train.pkl\n",
      "loaded obj/y_test.pkl\n"
     ]
    }
   ],
   "source": [
    "x_train = FP.load_obj('x_train_boolean')\n",
    "x_test = FP.load_obj('x_test_boolean')\n",
    "y_train = FP.load_obj('y_train')\n",
    "y_test = FP.load_obj('y_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-Perform Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\Miniconda3\\envs\\tf2\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====\n",
      "LOGISTIC REGRESSION\n",
      "Accuracy:  0.8373826203607095\n",
      "Precision Score:  0.8301615798922801\n",
      "Recall Score:  0.7890784982935154\n",
      "F1 Score:  0.8090988626421698\n"
     ]
    }
   ],
   "source": [
    "model_lr = LogisticRegression().fit(x_train,y_train)\n",
    "y_test_pred = model_lr.predict(x_test)\n",
    "print('====\\nLOGISTIC REGRESSION')\n",
    "print('Accuracy: ', accuracy_score(y_test,y_test_pred))\n",
    "print('Precision Score: ', precision_score(y_test,y_test_pred))\n",
    "print('Recall Score: ', recall_score(y_test,y_test_pred))\n",
    "print('F1 Score: ', f1_score(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! The accuracy and recall score are quite high. However, now we're gonna finetune this model to make it even better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "As sklearn use the threshold of 0.5, now we're gonna try different threshold to see what if the perfomance is gained on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THRESHOLD  0.1\n",
      "Accuracy:  0.8190490386048591\n",
      "Precision Score:  0.7448630136986302\n",
      "Recall Score:  0.8907849829351536\n",
      "F1 Score:  0.8113148896487411\n",
      "---\n",
      "THRESHOLD  0.2\n",
      "Accuracy:  0.8293337308093606\n",
      "Precision Score:  0.7764013626509755\n",
      "Recall Score:  0.8556313993174062\n",
      "F1 Score:  0.8140931969475563\n",
      "---\n",
      "THRESHOLD  0.30000000000000004\n",
      "Accuracy:  0.8335072290952452\n",
      "Precision Score:  0.7957585644371942\n",
      "Recall Score:  0.8324232081911263\n",
      "F1 Score:  0.8136780650542118\n",
      "---\n",
      "THRESHOLD  0.4\n",
      "Accuracy:  0.8361901922790281\n",
      "Precision Score:  0.8136348064405619\n",
      "Recall Score:  0.810580204778157\n",
      "F1 Score:  0.8121046332706445\n",
      "---\n",
      "THRESHOLD  0.5\n",
      "Accuracy:  0.8373826203607095\n",
      "Precision Score:  0.8301615798922801\n",
      "Recall Score:  0.7890784982935154\n",
      "F1 Score:  0.8090988626421698\n",
      "---\n",
      "THRESHOLD  0.6\n",
      "Accuracy:  0.8358920852586078\n",
      "Precision Score:  0.843409688321442\n",
      "Recall Score:  0.7665529010238907\n",
      "F1 Score:  0.803146790631146\n",
      "---\n",
      "THRESHOLD  0.7000000000000001\n",
      "Accuracy:  0.831569533462513\n",
      "Precision Score:  0.8571428571428571\n",
      "Recall Score:  0.7372013651877133\n",
      "F1 Score:  0.7926605504587155\n",
      "---\n",
      "THRESHOLD  0.8\n",
      "Accuracy:  0.827843195707259\n",
      "Precision Score:  0.8759000423549343\n",
      "Recall Score:  0.70580204778157\n",
      "F1 Score:  0.7817047817047816\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "thresholds = np.arange(0.1,0.9,0.1)\n",
    "for thresh in thresholds:\n",
    "    print('THRESHOLD ',thresh)\n",
    "    y_test_pred = [1  if prob[1] > thresh else 0 for prob in model_lr.predict_proba(x_test)]\n",
    "    print('Accuracy: ', accuracy_score(y_test,y_test_pred))\n",
    "    print('Precision Score: ', precision_score(y_test,y_test_pred))\n",
    "    print('Recall Score: ', recall_score(y_test,y_test_pred))\n",
    "    print('F1 Score: ', f1_score(y_test,y_test_pred))\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the hyperparam is somewhere between 0.4 -> 0.5, we're going to investigate this range more specifically :D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THRESHOLD  0.4\n",
      "Accuracy:  0.8361901922790281\n",
      "Precision Score:  0.8136348064405619\n",
      "Recall Score:  0.810580204778157\n",
      "F1 Score:  0.8121046332706445\n",
      "---\n",
      "THRESHOLD  0.41000000000000003\n",
      "Accuracy:  0.8367864063198688\n",
      "Precision Score:  0.8164884442911349\n",
      "Recall Score:  0.8078498293515358\n",
      "F1 Score:  0.8121461657231086\n",
      "---\n",
      "THRESHOLD  0.42000000000000004\n",
      "Accuracy:  0.836935459830079\n",
      "Precision Score:  0.8185287994448299\n",
      "Recall Score:  0.8051194539249147\n",
      "F1 Score:  0.8117687543014453\n",
      "---\n",
      "THRESHOLD  0.43000000000000005\n",
      "Accuracy:  0.8376807273811299\n",
      "Precision Score:  0.8208434994771697\n",
      "Recall Score:  0.8037542662116041\n",
      "F1 Score:  0.8122090015519917\n",
      "---\n",
      "THRESHOLD  0.44000000000000006\n",
      "Accuracy:  0.8379788344015502\n",
      "Precision Score:  0.8225411270563528\n",
      "Recall Score:  0.8020477815699659\n",
      "F1 Score:  0.8121651978572664\n",
      "---\n",
      "THRESHOLD  0.45000000000000007\n",
      "Accuracy:  0.83782978089134\n",
      "Precision Score:  0.8233848314606742\n",
      "Recall Score:  0.8003412969283277\n",
      "F1 Score:  0.811699550017307\n",
      "---\n",
      "THRESHOLD  0.4600000000000001\n",
      "Accuracy:  0.8382769414219705\n",
      "Precision Score:  0.8258565877781703\n",
      "Recall Score:  0.7979522184300342\n",
      "F1 Score:  0.8116646415552856\n",
      "---\n",
      "THRESHOLD  0.4700000000000001\n",
      "Accuracy:  0.8381278879117603\n",
      "Precision Score:  0.8276474769012082\n",
      "Recall Score:  0.7948805460750853\n",
      "F1 Score:  0.810933147632312\n",
      "---\n",
      "THRESHOLD  0.4800000000000001\n",
      "Accuracy:  0.83782978089134\n",
      "Precision Score:  0.8284593437945792\n",
      "Recall Score:  0.7928327645051194\n",
      "F1 Score:  0.8102546215556331\n",
      "---\n",
      "THRESHOLD  0.4900000000000001\n",
      "Accuracy:  0.8370845133402892\n",
      "Precision Score:  0.8286225402504472\n",
      "Recall Score:  0.7904436860068259\n",
      "F1 Score:  0.8090829694323145\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "thresholds = np.arange(0.4,0.5,0.01)\n",
    "for thresh in thresholds:\n",
    "    print('THRESHOLD ',thresh)\n",
    "    y_test_pred = [1  if prob[1] > thresh else 0 for prob in model_lr.predict_proba(x_test)]\n",
    "    print('Accuracy: ', accuracy_score(y_test,y_test_pred))\n",
    "    print('Precision Score: ', precision_score(y_test,y_test_pred))\n",
    "    print('Recall Score: ', recall_score(y_test,y_test_pred))\n",
    "    print('F1 Score: ', f1_score(y_test,y_test_pred))\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the hyperparam is somewhere between 0.45 -> 0.47, we're going to investigate this range more specifically :D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THRESHOLD  0.46\n",
      "Accuracy:  0.8382769414219705\n",
      "Precision Score:  0.8258565877781703\n",
      "Recall Score:  0.7979522184300342\n",
      "F1 Score:  0.8116646415552856\n",
      "---\n",
      "THRESHOLD  0.461\n",
      "Accuracy:  0.8384259949321806\n",
      "Precision Score:  0.826148409893993\n",
      "Recall Score:  0.7979522184300342\n",
      "F1 Score:  0.8118055555555556\n",
      "---\n",
      "THRESHOLD  0.462\n",
      "Accuracy:  0.8385750484423908\n",
      "Precision Score:  0.8266713830916166\n",
      "Recall Score:  0.7976109215017065\n",
      "F1 Score:  0.811881188118812\n",
      "---\n",
      "THRESHOLD  0.463\n",
      "Accuracy:  0.8384259949321806\n",
      "Precision Score:  0.8266100495399858\n",
      "Recall Score:  0.7972696245733788\n",
      "F1 Score:  0.8116747741487144\n",
      "---\n",
      "THRESHOLD  0.464\n",
      "Accuracy:  0.8382769414219705\n",
      "Precision Score:  0.82701169797944\n",
      "Recall Score:  0.7962457337883959\n",
      "F1 Score:  0.8113371587549992\n",
      "---\n",
      "THRESHOLD  0.465\n",
      "Accuracy:  0.8384259949321806\n",
      "Precision Score:  0.827304964539007\n",
      "Recall Score:  0.7962457337883959\n",
      "F1 Score:  0.8114782608695652\n",
      "---\n",
      "THRESHOLD  0.466\n",
      "Accuracy:  0.8382769414219705\n",
      "Precision Score:  0.8272437034409365\n",
      "Recall Score:  0.7959044368600683\n",
      "F1 Score:  0.8112715254826927\n",
      "---\n",
      "THRESHOLD  0.467\n",
      "Accuracy:  0.8382769414219705\n",
      "Precision Score:  0.8274760383386581\n",
      "Recall Score:  0.7955631399317407\n",
      "F1 Score:  0.8112058465286236\n",
      "---\n",
      "THRESHOLD  0.468\n",
      "Accuracy:  0.8381278879117603\n",
      "Precision Score:  0.8274147727272727\n",
      "Recall Score:  0.7952218430034129\n",
      "F1 Score:  0.8109989557953359\n",
      "---\n",
      "THRESHOLD  0.46900000000000003\n",
      "Accuracy:  0.8381278879117603\n",
      "Precision Score:  0.8276474769012082\n",
      "Recall Score:  0.7948805460750853\n",
      "F1 Score:  0.810933147632312\n",
      "---\n",
      "THRESHOLD  0.47000000000000003\n",
      "Accuracy:  0.8381278879117603\n",
      "Precision Score:  0.8276474769012082\n",
      "Recall Score:  0.7948805460750853\n",
      "F1 Score:  0.810933147632312\n",
      "---\n",
      "THRESHOLD  0.47100000000000003\n",
      "Accuracy:  0.8381278879117603\n",
      "Precision Score:  0.8278805120910384\n",
      "Recall Score:  0.7945392491467577\n",
      "F1 Score:  0.8108672936259144\n",
      "---\n",
      "THRESHOLD  0.47200000000000003\n",
      "Accuracy:  0.8381278879117603\n",
      "Precision Score:  0.8278805120910384\n",
      "Recall Score:  0.7945392491467577\n",
      "F1 Score:  0.8108672936259144\n",
      "---\n",
      "THRESHOLD  0.47300000000000003\n",
      "Accuracy:  0.8381278879117603\n",
      "Precision Score:  0.8281138790035587\n",
      "Recall Score:  0.79419795221843\n",
      "F1 Score:  0.8108013937282229\n",
      "---\n",
      "THRESHOLD  0.47400000000000003\n",
      "Accuracy:  0.8381278879117603\n",
      "Precision Score:  0.8281138790035587\n",
      "Recall Score:  0.79419795221843\n",
      "F1 Score:  0.8108013937282229\n",
      "---\n",
      "THRESHOLD  0.47500000000000003\n",
      "Accuracy:  0.8381278879117603\n",
      "Precision Score:  0.8281138790035587\n",
      "Recall Score:  0.79419795221843\n",
      "F1 Score:  0.8108013937282229\n",
      "---\n",
      "THRESHOLD  0.47600000000000003\n",
      "Accuracy:  0.8381278879117603\n",
      "Precision Score:  0.8281138790035587\n",
      "Recall Score:  0.79419795221843\n",
      "F1 Score:  0.8108013937282229\n",
      "---\n",
      "THRESHOLD  0.47700000000000004\n",
      "Accuracy:  0.8381278879117603\n",
      "Precision Score:  0.8281138790035587\n",
      "Recall Score:  0.79419795221843\n",
      "F1 Score:  0.8108013937282229\n",
      "---\n",
      "THRESHOLD  0.47800000000000004\n",
      "Accuracy:  0.8379788344015502\n",
      "Precision Score:  0.8282864267901674\n",
      "Recall Score:  0.7935153583617748\n",
      "F1 Score:  0.8105281506013596\n",
      "---\n",
      "THRESHOLD  0.47900000000000004\n",
      "Accuracy:  0.8379788344015502\n",
      "Precision Score:  0.8285204991087344\n",
      "Recall Score:  0.7931740614334472\n",
      "F1 Score:  0.8104620749782041\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "thresholds = np.arange(0.46,0.48,0.001) \n",
    "for thresh in thresholds:\n",
    "    print('THRESHOLD ',thresh)\n",
    "    y_test_pred = [1  if prob[1] > thresh else 0 for prob in model_lr.predict_proba(x_test)]\n",
    "    print('Accuracy: ', accuracy_score(y_test,y_test_pred))\n",
    "    print('Precision Score: ', precision_score(y_test,y_test_pred))\n",
    "    print('Recall Score: ', recall_score(y_test,y_test_pred))\n",
    "    print('F1 Score: ', f1_score(y_test,y_test_pred))\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So at the end our best threshold is 0.465 which gave us accuracy **0.8384** and F1 score of **0.8114** :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
